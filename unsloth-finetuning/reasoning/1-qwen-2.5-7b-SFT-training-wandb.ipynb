{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14ae23f-2f94-41f9-bacf-9d966cb41c51",
   "metadata": {},
   "source": [
    "# ü¶• Fine-tuning Qwen 2.5 7B with Unsloth: A Step-by-Step Guide\n",
    "\n",
    "Hey there, ML enthusiasts! üëã Ready to dive into some seriously fun model fine-tuning? We're going to take Qwen 2.5 7B for a spin and teach it some new tricks using Unsloth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aca6e3-441e-41e3-88db-50a05966d021",
   "metadata": {},
   "source": [
    "## üìö What We're Building\n",
    "\n",
    "We are setting up a fine-tuning pipeline for the Qwen 2.5 7B model using the Chain of Thought (CoT) dataset from OpenO1-SFT. This will help the model become better at explaining its reasoning process - kind of like teaching it to show its work instead of just blurting out answers!\n",
    "\n",
    "## üéØ Prerequisites\n",
    "\n",
    "Before we jump in, make sure you have:\n",
    "- Python 3.8+\n",
    "- A GPU with at least 16GB VRAM (we're using an NVIDIA L4)\n",
    "- Basic understanding of transformers and PyTorch\n",
    "- A cup of coffee ‚òï (optional but recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964760c2-9a56-453d-a54d-7e2eb07ed2f0",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "\n",
    "First, let's set up our environment. Unsloth offers different installation methods depending on your setup:\n",
    "\n",
    "### Option 1: Automatic Installation (Recommended)\n",
    "\n",
    "# Run this in your terminal to get the optimal installation command for your system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348db9d4-ae1f-4fa9-a23d-f50f020348ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab42538-5ba4-4381-a2b4-36d894e11221",
   "metadata": {},
   "source": [
    "### Option 2: Manual Installation\n",
    "Choose the appropriate command based on your PyTorch and CUDA versions:\n",
    "\n",
    "# First, upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ddad0-3b97-4edf-8b42-5f382989085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac2029-07d4-4449-b039-69a6a4da4c2a",
   "metadata": {},
   "source": [
    "# Then install Unsloth based on our setup:\n",
    "# For PyTorch 2.5 + CUDA 12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e857658-e5df-4cdd-981a-66fae66d3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bd019-139d-4c77-9911-932b9fa2ce06",
   "metadata": {},
   "source": [
    "# Install other required packages (if you plan to write your own training script, you can skip these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6fcf7-90a4-4a9e-8a65-591300fab864",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb scikit-learn evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02d42d-d6a7-40e4-ab6a-99c408e03c27",
   "metadata": {},
   "source": [
    "> üí° **Pro Tip**: Not sure which version to install? Use Option 1 (automatic installation) - it'll detect your setup and give you the right command!\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: Make sure you have CUDA toolkit installed on your system. The commands above assume you're using a CUDA-capable GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f641d3-97bd-4e13-9cf3-1c49ed5907a7",
   "metadata": {},
   "source": [
    "Now that we have our environment set up, let's dive into the fun part - getting our model ready! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d5e5b-e6fb-4f4d-9a1d-9f0b9cb5d62b",
   "metadata": {},
   "source": [
    "## üé¨ Step 1: Model Setup (`model_setup.py`)\n",
    "\n",
    "Let's start with the basics - getting our model ready! Create `model_setup.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bcdca-7985-4d7c-b1f5-db00841c1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "max_seq_length = 2048  # Flexible length - Unsloth handles RoPE scaling internally!\n",
    "dtype = None  # Auto-detection (Float16 for T4/V100, Bfloat16 for Ampere+)\n",
    "load_in_4bit = False  # 4-bit quantization option for memory savings\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"ü¶• Model loaded successfully! Ready to learn new tricks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088b989-8d33-4624-9f30-4240063f0442",
   "metadata": {},
   "source": [
    "## üé≠ Step 2: Dataset Preparation (`dataset.py`)\n",
    "\n",
    "Now that we have our model, let's prepare our dataset! Create `dataset.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba181dc7-27b4-4394-9522-57a681478846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Import model setup (reusing our previous code)\n",
    "max_seq_length = 2048\n",
    "dtype = None \n",
    "load_in_4bit = False\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Load our Chain of Thought dataset\n",
    "dataset = load_dataset(\"O1-OPEN/OpenO1-SFT\", split=\"train\")\n",
    "print(\"üìö Dataset columns:\", dataset.column_names)\n",
    "\n",
    "# Define our awesome prompt template\n",
    "instruction_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Function to format our prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        formatted_text = instruction_template.format(\n",
    "            instruction.strip(),\n",
    "            output.strip()\n",
    "        ) + tokenizer.eos_token\n",
    "        texts.append(formatted_text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Process the dataset\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func, \n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"üéâ Dataset processed and ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce8833-60af-47f4-9f1e-7b14c10eaf9a",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Training Setup (`qwen-2.5-7b-SFT-training-wandb.py`)\n",
    "\n",
    "Finally, the main event! Let's set up our training pipeline with all the bells and whistles. With bells and whistles I mean that transitioning from dataset.py to this allows you to appreciate the complexity of the final pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2ad8b-a7e3-4e0a-9a2e-49a1158a9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from unsloth import FastLanguageModel\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb for experiment tracking\n",
    "wandb.init(\n",
    "    project=\"qwen_cot_finetune\",\n",
    "    config={\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"architecture\": \"Qwen2.5-7B\",\n",
    "        \"dataset\": \"O1-OPEN/OpenO1-SFT\",\n",
    "        \"epochs\": 3,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up model and tokenizer (from model_setup.py)\n",
    "max_seq_length = 2048\n",
    "dtype = None \n",
    "load_in_4bit = False\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Load and prepare datasets (from dataset.py)\n",
    "train_dataset = load_dataset(\"O1-OPEN/OpenO1-SFT\", split=\"train[:80%]\")\n",
    "eval_dataset = load_dataset(\"O1-OPEN/OpenO1-SFT\", split=\"train[80%:]\")\n",
    "print(\"üìä Dataset splits ready!\")\n",
    "\n",
    "# Define our prompt template (from dataset.py)\n",
    "instruction_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        formatted_text = instruction_template.format(\n",
    "            instruction.strip(),\n",
    "            output.strip()\n",
    "        ) + tokenizer.eos_token\n",
    "        texts.append(formatted_text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Process datasets\n",
    "train_dataset = train_dataset.map(\n",
    "    formatting_prompts_func, \n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    formatting_prompts_func, \n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=6,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"logs\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"wandb\",\n",
    "    max_grad_norm=1.0,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    run_name=\"qwen_cot_finetune\"\n",
    ")\n",
    "\n",
    "# Initialize our trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=4,\n",
    "    packing=True,\n",
    "    args=training_args,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Let's get this party started! üéâ\n",
    "print(\"üöÄ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Clean up\n",
    "wandb.finish()\n",
    "print(\"‚ú® Training complete! Time to test our newly trained model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c9641-2942-40e1-87ef-f15e27855981",
   "metadata": {},
   "source": [
    "## üìà Monitoring Training\n",
    "\n",
    "During training, you can:\n",
    "1. Watch the training progress in your terminal\n",
    "2. Monitor metrics in real-time on Weights & Biases\n",
    "3. Check the `outputs` directory for saved checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb276f9b-cd3e-4e8b-9118-5ea6d9e6a99c",
   "metadata": {},
   "source": [
    "## üéâ What's Next?\n",
    "\n",
    "After training, you can:\n",
    "- Evaluate your model on specific tasks\n",
    "- Share your fine-tuned model on Hugging Face Hub\n",
    "- Experiment with different hyperparameters\n",
    "- Try different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3fa78-132c-4290-b769-8cda9eec4cd2",
   "metadata": {},
   "source": [
    "## ü§ù Contributing\n",
    "\n",
    "Found a bug? Have a suggestion? PRs are welcome! Just remember to:\n",
    "1. Fork the repository\n",
    "2. Create your feature branch\n",
    "3. Commit your changes\n",
    "4. Push to the branch\n",
    "5. Open a Pull Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2c778-b7d0-4386-8e7d-f0c4e733bafc",
   "metadata": {},
   "source": [
    "## üìù License\n",
    "\n",
    "This project is licensed under the Apache License 2.0 - see the LICENSE file for details.\n",
    "\n",
    "---\n",
    "\n",
    "Happy fine-tuning! Remember, if your model isn't learning, try turning it off and on again (just kidding, but sometimes it feels like that would help, right? üòÖ).\n",
    "\n",
    "For questions or issues, feel free to open an issue in the repository. And don't forget to ‚≠ê the repo if you found it helpful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
